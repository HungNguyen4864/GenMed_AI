{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","mount_file_id":"1sxt8ua3enoHBcC5Aim9Yi2J-AlEaiweA","authorship_tag":"ABX9TyM75FlVKNM0codSR1jWtY1y"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8Rvpm1G95OtR","executionInfo":{"status":"ok","timestamp":1734945643652,"user_tz":-420,"elapsed":26888,"user":{"displayName":"Hung Nguyen","userId":"03464478727849698728"}},"outputId":"e56ade3b-8cf9-4549-db0a-3f28a1d3c17e"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["import os\n","os.chdir(\"/content/drive/MyDrive/GSET25\")\n","!pwd"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nTQRTiPz5PBZ","executionInfo":{"status":"ok","timestamp":1734945643652,"user_tz":-420,"elapsed":7,"user":{"displayName":"Hung Nguyen","userId":"03464478727849698728"}},"outputId":"733be4c3-7a41-4f59-d7f2-ce2981041d02"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/GSET25\n"]}]},{"cell_type":"code","source":["!pip install medmnist\n","!pip install monai-generative\n","!pip install lpips"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"S3j7PwzGbRp_","executionInfo":{"status":"ok","timestamp":1734945656762,"user_tz":-420,"elapsed":13114,"user":{"displayName":"Hung Nguyen","userId":"03464478727849698728"}},"outputId":"92d34615-41c6-43c9-adb0-4ceeb32d0e82"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting medmnist\n","  Downloading medmnist-3.0.2-py3-none-any.whl.metadata (14 kB)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from medmnist) (1.26.4)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from medmnist) (2.2.2)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from medmnist) (1.6.0)\n","Requirement already satisfied: scikit-image in /usr/local/lib/python3.10/dist-packages (from medmnist) (0.25.0)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from medmnist) (4.67.1)\n","Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from medmnist) (11.0.0)\n","Collecting fire (from medmnist)\n","  Downloading fire-0.7.0.tar.gz (87 kB)\n","\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/87.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.2/87.2 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from medmnist) (2.5.1+cu121)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from medmnist) (0.20.1+cu121)\n","Requirement already satisfied: termcolor in /usr/local/lib/python3.10/dist-packages (from fire->medmnist) (2.5.0)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->medmnist) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->medmnist) (2024.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->medmnist) (2024.2)\n","Requirement already satisfied: scipy>=1.11.2 in /usr/local/lib/python3.10/dist-packages (from scikit-image->medmnist) (1.13.1)\n","Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.10/dist-packages (from scikit-image->medmnist) (3.4.2)\n","Requirement already satisfied: imageio!=2.35.0,>=2.33 in /usr/local/lib/python3.10/dist-packages (from scikit-image->medmnist) (2.36.1)\n","Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.10/dist-packages (from scikit-image->medmnist) (2024.12.12)\n","Requirement already satisfied: packaging>=21 in /usr/local/lib/python3.10/dist-packages (from scikit-image->medmnist) (24.2)\n","Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.10/dist-packages (from scikit-image->medmnist) (0.4)\n","Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->medmnist) (1.4.2)\n","Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->medmnist) (3.5.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->medmnist) (3.16.1)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->medmnist) (4.12.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->medmnist) (3.1.4)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->medmnist) (2024.10.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch->medmnist) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch->medmnist) (1.3.0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->medmnist) (1.17.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->medmnist) (3.0.2)\n","Downloading medmnist-3.0.2-py3-none-any.whl (25 kB)\n","Building wheels for collected packages: fire\n","  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for fire: filename=fire-0.7.0-py3-none-any.whl size=114249 sha256=331c24485028836607feca15b6048ffce21d798289ece423d0767d30f129a108\n","  Stored in directory: /root/.cache/pip/wheels/19/39/2f/2d3cadc408a8804103f1c34ddd4b9f6a93497b11fa96fe738e\n","Successfully built fire\n","Installing collected packages: fire, medmnist\n","Successfully installed fire-0.7.0 medmnist-3.0.2\n","Collecting monai-generative\n","  Downloading monai_generative-0.2.3-py3-none-any.whl.metadata (4.6 kB)\n","Collecting monai>=1.3.0 (from monai-generative)\n","  Downloading monai-1.4.0-py3-none-any.whl.metadata (11 kB)\n","Requirement already satisfied: numpy<2.0,>=1.24 in /usr/local/lib/python3.10/dist-packages (from monai>=1.3.0->monai-generative) (1.26.4)\n","Requirement already satisfied: torch>=1.9 in /usr/local/lib/python3.10/dist-packages (from monai>=1.3.0->monai-generative) (2.5.1+cu121)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.9->monai>=1.3.0->monai-generative) (3.16.1)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.9->monai>=1.3.0->monai-generative) (4.12.2)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.9->monai>=1.3.0->monai-generative) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.9->monai>=1.3.0->monai-generative) (3.1.4)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.9->monai>=1.3.0->monai-generative) (2024.10.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.9->monai>=1.3.0->monai-generative) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.9->monai>=1.3.0->monai-generative) (1.3.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.9->monai>=1.3.0->monai-generative) (3.0.2)\n","Downloading monai_generative-0.2.3-py3-none-any.whl (176 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.2/176.2 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading monai-1.4.0-py3-none-any.whl (1.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m40.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: monai, monai-generative\n","Successfully installed monai-1.4.0 monai-generative-0.2.3\n","Collecting lpips\n","  Downloading lpips-0.1.4-py3-none-any.whl.metadata (10 kB)\n","Requirement already satisfied: torch>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from lpips) (2.5.1+cu121)\n","Requirement already satisfied: torchvision>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from lpips) (0.20.1+cu121)\n","Requirement already satisfied: numpy>=1.14.3 in /usr/local/lib/python3.10/dist-packages (from lpips) (1.26.4)\n","Requirement already satisfied: scipy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from lpips) (1.13.1)\n","Requirement already satisfied: tqdm>=4.28.1 in /usr/local/lib/python3.10/dist-packages (from lpips) (4.67.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.0->lpips) (3.16.1)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.0->lpips) (4.12.2)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.0->lpips) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.0->lpips) (3.1.4)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.0->lpips) (2024.10.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.0->lpips) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=0.4.0->lpips) (1.3.0)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision>=0.2.1->lpips) (11.0.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=0.4.0->lpips) (3.0.2)\n","Downloading lpips-0.1.4-py3-none-any.whl (53 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.8/53.8 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: lpips\n","Successfully installed lpips-0.1.4\n"]}]},{"cell_type":"code","source":["import numpy as np\n","from medmnist import PneumoniaMNIST, ChestMNIST, BloodMNIST\n","from torch.utils.data import DataLoader, Subset\n","import matplotlib.pyplot as plt\n","from torchvision.transforms import Compose, ToTensor\n","\n","\n","\n","class MyMedMNIST(BloodMNIST):\n","    def __getitem__(self, item):\n","        img, _ = super().__getitem__(item)\n","        return img\n","\n","if __name__ == '__main__':\n","    image_size = 64\n","    train_data = MyMedMNIST(split=\"train\", download=True, size=image_size,root=\"/content/drive/MyDrive/GSET25\", transform=ToTensor())\n","    print(train_data)\n","    # indices = list(range(4000))\n","    # train_data = Subset(train_data, indices)\n","    print(len(train_data))\n","\n","    val_data = MyMedMNIST(split=\"val\", download=True, size=image_size,root=\"/content/drive/MyDrive/GSET25\" ,transform=ToTensor())\n","    print(val_data)\n","    # val_data = Subset(val_data, list(range(500)))\n","    print(len(val_data))\n","\n","    test_data = MyMedMNIST(split=\"test\", download=True, size=image_size,root=\"/content/drive/MyDrive/GSET25\", transform=ToTensor())\n","    print(test_data)\n","    # test_data = Subset(test_data, list(range(500)))\n","    print(len(test_data))\n","\n","    train_loader = DataLoader(\n","        dataset=train_data,\n","        batch_size=128,\n","        num_workers=2,\n","        shuffle=True,\n","        drop_last=False\n","    )\n","    val_loader = DataLoader(\n","        dataset=val_data,\n","        batch_size=128,\n","        num_workers=2,\n","        shuffle=True,\n","        drop_last=False\n","    )\n","    test_loader = DataLoader(\n","        dataset=test_data,\n","        batch_size=128,\n","        num_workers=2,\n","        shuffle=True,\n","        drop_last=False\n","    )"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rf6Z2J9TbV-n","executionInfo":{"status":"ok","timestamp":1734945687816,"user_tz":-420,"elapsed":31058,"user":{"displayName":"Hung Nguyen","userId":"03464478727849698728"}},"outputId":"9126c3f5-640a-450e-edea-e121b6e24f1d"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Using downloaded and verified file: /content/drive/MyDrive/GSET25/bloodmnist_64.npz\n","Dataset MyMedMNIST of size 64 (bloodmnist_64)\n","    Number of datapoints: 11959\n","    Root location: /content/drive/MyDrive/GSET25\n","    Split: train\n","    Task: multi-class\n","    Number of channels: 3\n","    Meaning of labels: {'0': 'basophil', '1': 'eosinophil', '2': 'erythroblast', '3': 'immature granulocytes(myelocytes, metamyelocytes and promyelocytes)', '4': 'lymphocyte', '5': 'monocyte', '6': 'neutrophil', '7': 'platelet'}\n","    Number of samples: {'train': 11959, 'val': 1712, 'test': 3421}\n","    Description: The BloodMNIST is based on a dataset of individual normal cells, captured from individuals without infection, hematologic or oncologic disease and free of any pharmacologic treatment at the moment of blood collection. It contains a total of 17,092 images and is organized into 8 classes. We split the source dataset with a ratio of 7:1:2 into training, validation and test set. The source images with resolution 3×360×363 pixels are center-cropped into 3×200×200, and then resized into 3×28×28.\n","    License: CC BY 4.0\n","11959\n","Using downloaded and verified file: /content/drive/MyDrive/GSET25/bloodmnist_64.npz\n","Dataset MyMedMNIST of size 64 (bloodmnist_64)\n","    Number of datapoints: 1712\n","    Root location: /content/drive/MyDrive/GSET25\n","    Split: val\n","    Task: multi-class\n","    Number of channels: 3\n","    Meaning of labels: {'0': 'basophil', '1': 'eosinophil', '2': 'erythroblast', '3': 'immature granulocytes(myelocytes, metamyelocytes and promyelocytes)', '4': 'lymphocyte', '5': 'monocyte', '6': 'neutrophil', '7': 'platelet'}\n","    Number of samples: {'train': 11959, 'val': 1712, 'test': 3421}\n","    Description: The BloodMNIST is based on a dataset of individual normal cells, captured from individuals without infection, hematologic or oncologic disease and free of any pharmacologic treatment at the moment of blood collection. It contains a total of 17,092 images and is organized into 8 classes. We split the source dataset with a ratio of 7:1:2 into training, validation and test set. The source images with resolution 3×360×363 pixels are center-cropped into 3×200×200, and then resized into 3×28×28.\n","    License: CC BY 4.0\n","1712\n","Using downloaded and verified file: /content/drive/MyDrive/GSET25/bloodmnist_64.npz\n","Dataset MyMedMNIST of size 64 (bloodmnist_64)\n","    Number of datapoints: 3421\n","    Root location: /content/drive/MyDrive/GSET25\n","    Split: test\n","    Task: multi-class\n","    Number of channels: 3\n","    Meaning of labels: {'0': 'basophil', '1': 'eosinophil', '2': 'erythroblast', '3': 'immature granulocytes(myelocytes, metamyelocytes and promyelocytes)', '4': 'lymphocyte', '5': 'monocyte', '6': 'neutrophil', '7': 'platelet'}\n","    Number of samples: {'train': 11959, 'val': 1712, 'test': 3421}\n","    Description: The BloodMNIST is based on a dataset of individual normal cells, captured from individuals without infection, hematologic or oncologic disease and free of any pharmacologic treatment at the moment of blood collection. It contains a total of 17,092 images and is organized into 8 classes. We split the source dataset with a ratio of 7:1:2 into training, validation and test set. The source images with resolution 3×360×363 pixels are center-cropped into 3×200×200, and then resized into 3×28×28.\n","    License: CC BY 4.0\n","3421\n"]}]},{"cell_type":"code","source":["import os\n","import shutil\n","import tempfile\n","import time\n","\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import torch\n","import torch.nn.functional as F\n","\n","import torch\n","import torch.nn.functional as F\n","import torchvision\n","from torchvision import transforms\n","from PIL import Image\n","from scipy import linalg\n","import pathlib\n","from INCEPTION import InceptionV3\n","\n","from monai import transforms\n","from monai.apps import MedNISTDataset\n","from monai.config import print_config\n","from monai.data import DataLoader, Dataset\n","from monai.utils import first, set_determinism\n","from torch.cuda.amp import GradScaler, autocast\n","from tqdm import tqdm\n","\n","from generative.inferers import LatentDiffusionInferer\n","from generative.losses.adversarial_loss import PatchAdversarialLoss\n","from generative.losses.perceptual import PerceptualLoss\n","from generative.networks.nets import DiffusionModelUNet, PatchDiscriminator\n","from generative.networks.schedulers import DDPMScheduler\n","from generative_custom.networks.nets import AutoencoderKL\n"],"metadata":{"id":"o0Jwlqf-cF84","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1734945721964,"user_tz":-420,"elapsed":34153,"user":{"displayName":"Hung Nguyen","userId":"03464478727849698728"}},"outputId":"0ae75a3d-c8aa-4dba-913e-c68a100df0f7"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/generative/networks/layers/vector_quantizer.py:86: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  @torch.cuda.amp.autocast(enabled=False)\n","/usr/local/lib/python3.10/dist-packages/generative/networks/layers/vector_quantizer.py:124: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  @torch.cuda.amp.autocast(enabled=False)\n"]}]},{"cell_type":"code","source":["set_determinism(42)\n","directory = os.environ.get(\"MONAI_DATA_DIRECTORY\")\n","root_dir = tempfile.mkdtemp() if directory is None else directory\n","print(root_dir)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4msOqqdrYbTn","executionInfo":{"status":"ok","timestamp":1734770569934,"user_tz":-420,"elapsed":10,"user":{"displayName":"Hung Nguyen","userId":"03464478727849698728"}},"outputId":"80600967-d3bb-4b1d-d50f-cbce0d6ff05c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/tmp/tmpfy60jtzs\n"]}]},{"cell_type":"code","source":["val_data = MedNISTDataset(root_dir=root_dir, section=\"validation\", download=True, seed=0)\n","val_datalist = [{\"image\": item[\"image\"]} for item in val_data.data if item[\"class_name\"] == \"Hand\"]\n","val_transforms = transforms.Compose(\n","    [\n","        transforms.LoadImaged(keys=[\"image\"]),\n","        transforms.EnsureChannelFirstd(keys=[\"image\"]),\n","        transforms.ScaleIntensityRanged(keys=[\"image\"], a_min=0.0, a_max=255.0, b_min=0.0, b_max=1.0, clip=True),\n","    ]\n",")\n","val_ds = Dataset(data=val_datalist, transform=val_transforms)\n","val_loader = DataLoader(val_ds, batch_size=64, shuffle=True, num_workers=4, persistent_workers=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6_-GCbmZYRP-","executionInfo":{"status":"ok","timestamp":1734770588756,"user_tz":-420,"elapsed":18830,"user":{"displayName":"Hung Nguyen","userId":"03464478727849698728"}},"outputId":"b6ffc883-45f5-4d6e-e2be-be872ad177e7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["MedNIST.tar.gz: 59.0MB [00:01, 55.2MB/s]                            "]},{"output_type":"stream","name":"stdout","text":["2024-12-21 08:42:49,591 - INFO - Downloaded: /tmp/tmpfy60jtzs/MedNIST.tar.gz\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["2024-12-21 08:42:49,702 - INFO - Verified 'MedNIST.tar.gz', md5: 0bc7306e7427e00ad1c5526a6677552d.\n","2024-12-21 08:42:49,703 - INFO - Writing into directory: /tmp/tmpfy60jtzs.\n"]},{"output_type":"stream","name":"stderr","text":["Loading dataset: 100%|██████████| 5895/5895 [00:04<00:00, 1368.26it/s]\n","/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(\n"]}]},{"cell_type":"code","source":["train_data = MedNISTDataset(root_dir=root_dir, section=\"training\", download=True, seed=0)\n","train_datalist = [{\"image\": item[\"image\"]} for item in train_data.data if item[\"class_name\"] == \"Hand\"]\n","image_size = 64\n","train_transforms = transforms.Compose(\n","    [\n","        transforms.LoadImaged(keys=[\"image\"]),\n","        transforms.EnsureChannelFirstd(keys=[\"image\"]),\n","        transforms.ScaleIntensityRanged(keys=[\"image\"], a_min=0.0, a_max=255.0, b_min=0.0, b_max=1.0, clip=True),\n","        transforms.RandAffined(\n","            keys=[\"image\"],\n","            rotate_range=[(-np.pi / 36, np.pi / 36), (-np.pi / 36, np.pi / 36)],\n","            translate_range=[(-1, 1), (-1, 1)],\n","            scale_range=[(-0.05, 0.05), (-0.05, 0.05)],\n","            spatial_size=[image_size, image_size],\n","            padding_mode=\"zeros\",\n","            prob=0.5,\n","        ),\n","    ]\n",")\n","train_ds = Dataset(data=train_datalist, transform=train_transforms)\n","train_loader = DataLoader(train_ds, batch_size=64, shuffle=True, num_workers=4, persistent_workers=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ktt1TddEYqE3","executionInfo":{"status":"ok","timestamp":1734770643348,"user_tz":-420,"elapsed":41412,"user":{"displayName":"Hung Nguyen","userId":"03464478727849698728"}},"outputId":"5d7405a7-0434-4b6c-e80b-99b271ed9191"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["2024-12-21 08:43:20,966 - INFO - Verified 'MedNIST.tar.gz', md5: 0bc7306e7427e00ad1c5526a6677552d.\n","2024-12-21 08:43:20,968 - INFO - File exists: /tmp/tmpfy60jtzs/MedNIST.tar.gz, skipped downloading.\n","2024-12-21 08:43:20,971 - INFO - Non-empty folder exists in /tmp/tmpfy60jtzs/MedNIST, skipped extracting.\n"]},{"output_type":"stream","name":"stderr","text":["Loading dataset: 100%|██████████| 47164/47164 [00:40<00:00, 1168.55it/s]\n"]}]},{"cell_type":"code","source":["check_data = next(iter((train_loader)))\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","# autoencoderkl = AutoencoderKL(\n","#     spatial_dims=2,\n","#     in_channels=3,\n","#     out_channels=3,\n","#     num_channels=(128, 128, 256),\n","#     latent_channels=3,\n","#     num_res_blocks=2,\n","#     attention_levels=(False, False, False),\n","#     with_encoder_nonlocal_attn=False,\n","#     with_decoder_nonlocal_attn=False,\n","# )\n","autoencoderkl = AutoencoderKL(\n","    spatial_dims=2,\n","    in_channels=3,\n","    out_channels=3,\n","    num_channels=(128, 128, 256),\n","    latent_channels=3,\n","    num_res_blocks=2,\n","    attention_levels=(False, True, True),\n","    with_encoder_nonlocal_attn=False,\n","    with_decoder_nonlocal_attn=False,\n",")\n","autoencoderkl = autoencoderkl.to(device)\n","# unet = DiffusionModelUNet(\n","#     spatial_dims=2,\n","#     in_channels=3,\n","#     out_channels=3,\n","#     num_res_blocks=2,\n","#     num_channels=(128, 256, 512),\n","#     attention_levels=(False, True, True),\n","#     num_head_channels=(0, 256, 512),\n","# )\n","unet = DiffusionModelUNet(\n","    spatial_dims=2,\n","    in_channels=3,\n","    out_channels=3,\n","    num_res_blocks=2,\n","    num_channels=(128, 256, 512),\n","    attention_levels=(True, True, True),\n","    num_head_channels=(128, 256, 512),\n",")\n","\n","with torch.no_grad():\n","    with autocast(enabled=True):\n","        z = autoencoderkl.encode_stage_2_inputs(check_data.to(device))\n","print(f\"Scaling factor set to {1/torch.std(z)}\")\n","scale_factor = 1 / torch.std(z)\n","scheduler = DDPMScheduler(num_train_timesteps=1000, schedule=\"linear_beta\", beta_start=0.0015, beta_end=0.0195)\n","inferer = LatentDiffusionInferer(scheduler, scale_factor=scale_factor)\n","unet = unet.to(device)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PzFWYJ6acUup","executionInfo":{"status":"ok","timestamp":1734946064587,"user_tz":-420,"elapsed":3581,"user":{"displayName":"Hung Nguyen","userId":"03464478727849698728"}},"outputId":"8ac4da37-336b-4d9e-9f16-91085f4d1d85"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-8-431240a13629>:46: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with autocast(enabled=True):\n"]},{"output_type":"stream","name":"stdout","text":["Scaling factor set to 0.8932662606239319\n"]}]},{"cell_type":"code","source":["# check_point = torch.load('/content/drive/MyDrive/GSET25/AEKL_Hand_Sep.pt')\n","# autoencoderkl.load_state_dict(check_point[\"autoencoderkl\"])\n","# check_point = torch.load('/content/drive/MyDrive/GSET25/uNet_Hand_Sep.pt')\n","# unet.load_state_dict(check_point[\"unet\"],strict=False)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BXTvarcLu-5Y","executionInfo":{"status":"ok","timestamp":1734773660859,"user_tz":-420,"elapsed":23830,"user":{"displayName":"Hung Nguyen","userId":"03464478727849698728"}},"outputId":"e9aa6f6f-0512-4928-8513-6b8c77db1c55"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-20-138698489675>:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  check_point = torch.load('/content/drive/MyDrive/GSET25/AEKL_Hand_Sep.pt')\n","<ipython-input-20-138698489675>:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  check_point = torch.load('/content/drive/MyDrive/GSET25/uNet_Hand_Sep.pt')\n"]},{"output_type":"execute_result","data":{"text/plain":["_IncompatibleKeys(missing_keys=['down_blocks.0.attentions.0.to_q.weight', 'down_blocks.0.attentions.0.to_q.bias', 'down_blocks.0.attentions.0.to_k.weight', 'down_blocks.0.attentions.0.to_k.bias', 'down_blocks.0.attentions.0.to_v.weight', 'down_blocks.0.attentions.0.to_v.bias', 'down_blocks.0.attentions.0.proj_attn.weight', 'down_blocks.0.attentions.0.proj_attn.bias', 'down_blocks.0.attentions.1.to_q.weight', 'down_blocks.0.attentions.1.to_q.bias', 'down_blocks.0.attentions.1.to_k.weight', 'down_blocks.0.attentions.1.to_k.bias', 'down_blocks.0.attentions.1.to_v.weight', 'down_blocks.0.attentions.1.to_v.bias', 'down_blocks.0.attentions.1.proj_attn.weight', 'down_blocks.0.attentions.1.proj_attn.bias', 'down_blocks.1.attentions.0.to_q.weight', 'down_blocks.1.attentions.0.to_q.bias', 'down_blocks.1.attentions.0.to_k.weight', 'down_blocks.1.attentions.0.to_k.bias', 'down_blocks.1.attentions.0.to_v.weight', 'down_blocks.1.attentions.0.to_v.bias', 'down_blocks.1.attentions.0.proj_attn.weight', 'down_blocks.1.attentions.0.proj_attn.bias', 'down_blocks.1.attentions.1.to_q.weight', 'down_blocks.1.attentions.1.to_q.bias', 'down_blocks.1.attentions.1.to_k.weight', 'down_blocks.1.attentions.1.to_k.bias', 'down_blocks.1.attentions.1.to_v.weight', 'down_blocks.1.attentions.1.to_v.bias', 'down_blocks.1.attentions.1.proj_attn.weight', 'down_blocks.1.attentions.1.proj_attn.bias', 'down_blocks.2.attentions.0.to_q.weight', 'down_blocks.2.attentions.0.to_q.bias', 'down_blocks.2.attentions.0.to_k.weight', 'down_blocks.2.attentions.0.to_k.bias', 'down_blocks.2.attentions.0.to_v.weight', 'down_blocks.2.attentions.0.to_v.bias', 'down_blocks.2.attentions.0.proj_attn.weight', 'down_blocks.2.attentions.0.proj_attn.bias', 'down_blocks.2.attentions.1.to_q.weight', 'down_blocks.2.attentions.1.to_q.bias', 'down_blocks.2.attentions.1.to_k.weight', 'down_blocks.2.attentions.1.to_k.bias', 'down_blocks.2.attentions.1.to_v.weight', 'down_blocks.2.attentions.1.to_v.bias', 'down_blocks.2.attentions.1.proj_attn.weight', 'down_blocks.2.attentions.1.proj_attn.bias', 'middle_block.attention.to_q.weight', 'middle_block.attention.to_q.bias', 'middle_block.attention.to_k.weight', 'middle_block.attention.to_k.bias', 'middle_block.attention.to_v.weight', 'middle_block.attention.to_v.bias', 'middle_block.attention.proj_attn.weight', 'middle_block.attention.proj_attn.bias', 'up_blocks.0.attentions.0.to_q.weight', 'up_blocks.0.attentions.0.to_q.bias', 'up_blocks.0.attentions.0.to_k.weight', 'up_blocks.0.attentions.0.to_k.bias', 'up_blocks.0.attentions.0.to_v.weight', 'up_blocks.0.attentions.0.to_v.bias', 'up_blocks.0.attentions.0.proj_attn.weight', 'up_blocks.0.attentions.0.proj_attn.bias', 'up_blocks.0.attentions.1.to_q.weight', 'up_blocks.0.attentions.1.to_q.bias', 'up_blocks.0.attentions.1.to_k.weight', 'up_blocks.0.attentions.1.to_k.bias', 'up_blocks.0.attentions.1.to_v.weight', 'up_blocks.0.attentions.1.to_v.bias', 'up_blocks.0.attentions.1.proj_attn.weight', 'up_blocks.0.attentions.1.proj_attn.bias', 'up_blocks.0.attentions.2.to_q.weight', 'up_blocks.0.attentions.2.to_q.bias', 'up_blocks.0.attentions.2.to_k.weight', 'up_blocks.0.attentions.2.to_k.bias', 'up_blocks.0.attentions.2.to_v.weight', 'up_blocks.0.attentions.2.to_v.bias', 'up_blocks.0.attentions.2.proj_attn.weight', 'up_blocks.0.attentions.2.proj_attn.bias', 'up_blocks.1.attentions.0.to_q.weight', 'up_blocks.1.attentions.0.to_q.bias', 'up_blocks.1.attentions.0.to_k.weight', 'up_blocks.1.attentions.0.to_k.bias', 'up_blocks.1.attentions.0.to_v.weight', 'up_blocks.1.attentions.0.to_v.bias', 'up_blocks.1.attentions.0.proj_attn.weight', 'up_blocks.1.attentions.0.proj_attn.bias', 'up_blocks.1.attentions.1.to_q.weight', 'up_blocks.1.attentions.1.to_q.bias', 'up_blocks.1.attentions.1.to_k.weight', 'up_blocks.1.attentions.1.to_k.bias', 'up_blocks.1.attentions.1.to_v.weight', 'up_blocks.1.attentions.1.to_v.bias', 'up_blocks.1.attentions.1.proj_attn.weight', 'up_blocks.1.attentions.1.proj_attn.bias', 'up_blocks.1.attentions.2.to_q.weight', 'up_blocks.1.attentions.2.to_q.bias', 'up_blocks.1.attentions.2.to_k.weight', 'up_blocks.1.attentions.2.to_k.bias', 'up_blocks.1.attentions.2.to_v.weight', 'up_blocks.1.attentions.2.to_v.bias', 'up_blocks.1.attentions.2.proj_attn.weight', 'up_blocks.1.attentions.2.proj_attn.bias', 'up_blocks.2.attentions.0.to_q.weight', 'up_blocks.2.attentions.0.to_q.bias', 'up_blocks.2.attentions.0.to_k.weight', 'up_blocks.2.attentions.0.to_k.bias', 'up_blocks.2.attentions.0.to_v.weight', 'up_blocks.2.attentions.0.to_v.bias', 'up_blocks.2.attentions.0.proj_attn.weight', 'up_blocks.2.attentions.0.proj_attn.bias', 'up_blocks.2.attentions.1.to_q.weight', 'up_blocks.2.attentions.1.to_q.bias', 'up_blocks.2.attentions.1.to_k.weight', 'up_blocks.2.attentions.1.to_k.bias', 'up_blocks.2.attentions.1.to_v.weight', 'up_blocks.2.attentions.1.to_v.bias', 'up_blocks.2.attentions.1.proj_attn.weight', 'up_blocks.2.attentions.1.proj_attn.bias', 'up_blocks.2.attentions.2.to_q.weight', 'up_blocks.2.attentions.2.to_q.bias', 'up_blocks.2.attentions.2.to_k.weight', 'up_blocks.2.attentions.2.to_k.bias', 'up_blocks.2.attentions.2.to_v.weight', 'up_blocks.2.attentions.2.to_v.bias', 'up_blocks.2.attentions.2.proj_attn.weight', 'up_blocks.2.attentions.2.proj_attn.bias'], unexpected_keys=['down_blocks.0.attentions.0.proj_i.weight', 'down_blocks.0.attentions.0.proj_i.bias', 'down_blocks.0.attentions.0.proj_k.weight', 'down_blocks.0.attentions.0.proj_k.bias', 'down_blocks.0.attentions.0.proj_v.weight', 'down_blocks.0.attentions.0.proj_v.bias', 'down_blocks.0.attentions.0.fc.weight', 'down_blocks.0.attentions.0.fc.bias', 'down_blocks.0.attentions.1.proj_i.weight', 'down_blocks.0.attentions.1.proj_i.bias', 'down_blocks.0.attentions.1.proj_k.weight', 'down_blocks.0.attentions.1.proj_k.bias', 'down_blocks.0.attentions.1.proj_v.weight', 'down_blocks.0.attentions.1.proj_v.bias', 'down_blocks.0.attentions.1.fc.weight', 'down_blocks.0.attentions.1.fc.bias', 'down_blocks.1.attentions.0.proj_i.weight', 'down_blocks.1.attentions.0.proj_i.bias', 'down_blocks.1.attentions.0.proj_k.weight', 'down_blocks.1.attentions.0.proj_k.bias', 'down_blocks.1.attentions.0.proj_v.weight', 'down_blocks.1.attentions.0.proj_v.bias', 'down_blocks.1.attentions.0.fc.weight', 'down_blocks.1.attentions.0.fc.bias', 'down_blocks.1.attentions.1.proj_i.weight', 'down_blocks.1.attentions.1.proj_i.bias', 'down_blocks.1.attentions.1.proj_k.weight', 'down_blocks.1.attentions.1.proj_k.bias', 'down_blocks.1.attentions.1.proj_v.weight', 'down_blocks.1.attentions.1.proj_v.bias', 'down_blocks.1.attentions.1.fc.weight', 'down_blocks.1.attentions.1.fc.bias', 'down_blocks.2.attentions.0.proj_i.weight', 'down_blocks.2.attentions.0.proj_i.bias', 'down_blocks.2.attentions.0.proj_k.weight', 'down_blocks.2.attentions.0.proj_k.bias', 'down_blocks.2.attentions.0.proj_v.weight', 'down_blocks.2.attentions.0.proj_v.bias', 'down_blocks.2.attentions.0.fc.weight', 'down_blocks.2.attentions.0.fc.bias', 'down_blocks.2.attentions.1.proj_i.weight', 'down_blocks.2.attentions.1.proj_i.bias', 'down_blocks.2.attentions.1.proj_k.weight', 'down_blocks.2.attentions.1.proj_k.bias', 'down_blocks.2.attentions.1.proj_v.weight', 'down_blocks.2.attentions.1.proj_v.bias', 'down_blocks.2.attentions.1.fc.weight', 'down_blocks.2.attentions.1.fc.bias', 'middle_block.attention.proj_i.weight', 'middle_block.attention.proj_i.bias', 'middle_block.attention.proj_k.weight', 'middle_block.attention.proj_k.bias', 'middle_block.attention.proj_v.weight', 'middle_block.attention.proj_v.bias', 'middle_block.attention.fc.weight', 'middle_block.attention.fc.bias', 'up_blocks.0.attentions.0.proj_i.weight', 'up_blocks.0.attentions.0.proj_i.bias', 'up_blocks.0.attentions.0.proj_k.weight', 'up_blocks.0.attentions.0.proj_k.bias', 'up_blocks.0.attentions.0.proj_v.weight', 'up_blocks.0.attentions.0.proj_v.bias', 'up_blocks.0.attentions.0.fc.weight', 'up_blocks.0.attentions.0.fc.bias', 'up_blocks.0.attentions.1.proj_i.weight', 'up_blocks.0.attentions.1.proj_i.bias', 'up_blocks.0.attentions.1.proj_k.weight', 'up_blocks.0.attentions.1.proj_k.bias', 'up_blocks.0.attentions.1.proj_v.weight', 'up_blocks.0.attentions.1.proj_v.bias', 'up_blocks.0.attentions.1.fc.weight', 'up_blocks.0.attentions.1.fc.bias', 'up_blocks.0.attentions.2.proj_i.weight', 'up_blocks.0.attentions.2.proj_i.bias', 'up_blocks.0.attentions.2.proj_k.weight', 'up_blocks.0.attentions.2.proj_k.bias', 'up_blocks.0.attentions.2.proj_v.weight', 'up_blocks.0.attentions.2.proj_v.bias', 'up_blocks.0.attentions.2.fc.weight', 'up_blocks.0.attentions.2.fc.bias', 'up_blocks.1.attentions.0.proj_i.weight', 'up_blocks.1.attentions.0.proj_i.bias', 'up_blocks.1.attentions.0.proj_k.weight', 'up_blocks.1.attentions.0.proj_k.bias', 'up_blocks.1.attentions.0.proj_v.weight', 'up_blocks.1.attentions.0.proj_v.bias', 'up_blocks.1.attentions.0.fc.weight', 'up_blocks.1.attentions.0.fc.bias', 'up_blocks.1.attentions.1.proj_i.weight', 'up_blocks.1.attentions.1.proj_i.bias', 'up_blocks.1.attentions.1.proj_k.weight', 'up_blocks.1.attentions.1.proj_k.bias', 'up_blocks.1.attentions.1.proj_v.weight', 'up_blocks.1.attentions.1.proj_v.bias', 'up_blocks.1.attentions.1.fc.weight', 'up_blocks.1.attentions.1.fc.bias', 'up_blocks.1.attentions.2.proj_i.weight', 'up_blocks.1.attentions.2.proj_i.bias', 'up_blocks.1.attentions.2.proj_k.weight', 'up_blocks.1.attentions.2.proj_k.bias', 'up_blocks.1.attentions.2.proj_v.weight', 'up_blocks.1.attentions.2.proj_v.bias', 'up_blocks.1.attentions.2.fc.weight', 'up_blocks.1.attentions.2.fc.bias', 'up_blocks.2.attentions.0.proj_i.weight', 'up_blocks.2.attentions.0.proj_i.bias', 'up_blocks.2.attentions.0.proj_k.weight', 'up_blocks.2.attentions.0.proj_k.bias', 'up_blocks.2.attentions.0.proj_v.weight', 'up_blocks.2.attentions.0.proj_v.bias', 'up_blocks.2.attentions.0.fc.weight', 'up_blocks.2.attentions.0.fc.bias', 'up_blocks.2.attentions.1.proj_i.weight', 'up_blocks.2.attentions.1.proj_i.bias', 'up_blocks.2.attentions.1.proj_k.weight', 'up_blocks.2.attentions.1.proj_k.bias', 'up_blocks.2.attentions.1.proj_v.weight', 'up_blocks.2.attentions.1.proj_v.bias', 'up_blocks.2.attentions.1.fc.weight', 'up_blocks.2.attentions.1.fc.bias', 'up_blocks.2.attentions.2.proj_i.weight', 'up_blocks.2.attentions.2.proj_i.bias', 'up_blocks.2.attentions.2.proj_k.weight', 'up_blocks.2.attentions.2.proj_k.bias', 'up_blocks.2.attentions.2.proj_v.weight', 'up_blocks.2.attentions.2.proj_v.bias', 'up_blocks.2.attentions.2.fc.weight', 'up_blocks.2.attentions.2.fc.bias'])"]},"metadata":{},"execution_count":20}]},{"cell_type":"code","source":["unet.load_state_dict(torch.load('/content/drive/MyDrive/GSET25/unet_sep_model_blood.pth'))\n","scheduler.load_state_dict(torch.load('/content/drive/MyDrive/GSET25/scheduler_sep_model_blood.pth'))\n","autoencoderkl.load_state_dict(torch.load('/content/drive/MyDrive/GSET25/autoencoder_sep_model_blood.pth'))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7Nm3-RdibtoJ","executionInfo":{"status":"ok","timestamp":1734946084657,"user_tz":-420,"elapsed":9830,"user":{"displayName":"Hung Nguyen","userId":"03464478727849698728"}},"outputId":"41eb3d39-7f29-4142-c186-512989dd9da7"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-9-540a214b1547>:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  unet.load_state_dict(torch.load('/content/drive/MyDrive/GSET25/unet_sep_model_blood.pth'))\n","<ipython-input-9-540a214b1547>:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  scheduler.load_state_dict(torch.load('/content/drive/MyDrive/GSET25/scheduler_sep_model_blood.pth'))\n","<ipython-input-9-540a214b1547>:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  autoencoderkl.load_state_dict(torch.load('/content/drive/MyDrive/GSET25/autoencoder_sep_model_blood.pth'))\n"]},{"output_type":"execute_result","data":{"text/plain":["<All keys matched successfully>"]},"metadata":{},"execution_count":9}]},{"cell_type":"code","source":["list_inter = []\n","start_time = time.time()\n","for i in range(100):\n","  unet.eval()\n","  scheduler.set_timesteps(num_inference_steps=1000)\n","  noise = torch.randn((1, 3, 16, 16))\n","  noise = noise.to(device)\n","  with torch.no_grad():\n","      image, intermediates = inferer.sample(\n","          input_noise=noise,\n","          diffusion_model=unet,\n","          scheduler=scheduler,\n","          save_intermediates=True,\n","          intermediate_steps=100,\n","          autoencoder_model=autoencoderkl,\n","      )\n","  list_inter.append(intermediates)\n","elapsed_time = time.time() - start_time\n","print(f\"Total training time: {elapsed_time:.2f} seconds\")"],"metadata":{"id":"SY5fbi8BbQUQ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1734948172060,"user_tz":-420,"elapsed":2077875,"user":{"displayName":"Hung Nguyen","userId":"03464478727849698728"}},"outputId":"0da13d21-8f99-427a-f33a-46e85a3d3d05"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 1000/1000 [00:21<00:00, 45.46it/s]\n","100%|██████████| 1000/1000 [00:23<00:00, 42.84it/s]\n","100%|██████████| 1000/1000 [00:23<00:00, 42.58it/s]\n","100%|██████████| 1000/1000 [00:24<00:00, 41.02it/s]\n","100%|██████████| 1000/1000 [00:24<00:00, 40.11it/s]\n","100%|██████████| 1000/1000 [00:21<00:00, 46.56it/s]\n","100%|██████████| 1000/1000 [00:22<00:00, 45.14it/s]\n","100%|██████████| 1000/1000 [00:21<00:00, 46.28it/s]\n","100%|██████████| 1000/1000 [00:20<00:00, 48.95it/s]\n","100%|██████████| 1000/1000 [00:21<00:00, 46.66it/s]\n","100%|██████████| 1000/1000 [00:20<00:00, 48.13it/s]\n","100%|██████████| 1000/1000 [00:22<00:00, 45.08it/s]\n","100%|██████████| 1000/1000 [00:19<00:00, 50.39it/s]\n","100%|██████████| 1000/1000 [00:20<00:00, 48.95it/s]\n","100%|██████████| 1000/1000 [00:21<00:00, 47.15it/s]\n","100%|██████████| 1000/1000 [00:20<00:00, 49.58it/s]\n","100%|██████████| 1000/1000 [00:20<00:00, 47.93it/s]\n","100%|██████████| 1000/1000 [00:19<00:00, 50.51it/s]\n","100%|██████████| 1000/1000 [00:21<00:00, 47.43it/s]\n","100%|██████████| 1000/1000 [00:20<00:00, 49.62it/s]\n","100%|██████████| 1000/1000 [00:21<00:00, 45.49it/s]\n","100%|██████████| 1000/1000 [00:20<00:00, 49.04it/s]\n","100%|██████████| 1000/1000 [00:21<00:00, 46.98it/s]\n","100%|██████████| 1000/1000 [00:20<00:00, 49.15it/s]\n","100%|██████████| 1000/1000 [00:20<00:00, 48.95it/s]\n","100%|██████████| 1000/1000 [00:20<00:00, 49.08it/s]\n","100%|██████████| 1000/1000 [00:20<00:00, 49.74it/s]\n","100%|██████████| 1000/1000 [00:20<00:00, 47.75it/s]\n","100%|██████████| 1000/1000 [00:19<00:00, 50.72it/s]\n","100%|██████████| 1000/1000 [00:21<00:00, 46.82it/s]\n","100%|██████████| 1000/1000 [00:19<00:00, 50.15it/s]\n","100%|██████████| 1000/1000 [00:20<00:00, 48.53it/s]\n","100%|██████████| 1000/1000 [00:20<00:00, 48.94it/s]\n","100%|██████████| 1000/1000 [00:20<00:00, 48.53it/s]\n","100%|██████████| 1000/1000 [00:21<00:00, 47.44it/s]\n","100%|██████████| 1000/1000 [00:19<00:00, 50.47it/s]\n","100%|██████████| 1000/1000 [00:21<00:00, 47.11it/s]\n","100%|██████████| 1000/1000 [00:19<00:00, 50.48it/s]\n","100%|██████████| 1000/1000 [00:20<00:00, 48.17it/s]\n","100%|██████████| 1000/1000 [00:19<00:00, 50.48it/s]\n","100%|██████████| 1000/1000 [00:20<00:00, 48.46it/s]\n","100%|██████████| 1000/1000 [00:20<00:00, 48.30it/s]\n","100%|██████████| 1000/1000 [00:19<00:00, 50.32it/s]\n","100%|██████████| 1000/1000 [00:21<00:00, 47.27it/s]\n","100%|██████████| 1000/1000 [00:19<00:00, 50.52it/s]\n","100%|██████████| 1000/1000 [00:21<00:00, 47.32it/s]\n","100%|██████████| 1000/1000 [00:19<00:00, 50.26it/s]\n","100%|██████████| 1000/1000 [00:20<00:00, 47.76it/s]\n","100%|██████████| 1000/1000 [00:19<00:00, 50.03it/s]\n","100%|██████████| 1000/1000 [00:21<00:00, 47.03it/s]\n","100%|██████████| 1000/1000 [00:20<00:00, 49.51it/s]\n","100%|██████████| 1000/1000 [00:20<00:00, 47.99it/s]\n","100%|██████████| 1000/1000 [00:20<00:00, 47.71it/s]\n","100%|██████████| 1000/1000 [00:19<00:00, 50.23it/s]\n","100%|██████████| 1000/1000 [00:20<00:00, 47.74it/s]\n","100%|██████████| 1000/1000 [00:20<00:00, 48.90it/s]\n","100%|██████████| 1000/1000 [00:21<00:00, 46.61it/s]\n","100%|██████████| 1000/1000 [00:20<00:00, 49.96it/s]\n","100%|██████████| 1000/1000 [00:21<00:00, 47.08it/s]\n","100%|██████████| 1000/1000 [00:19<00:00, 50.21it/s]\n","100%|██████████| 1000/1000 [00:20<00:00, 49.48it/s]\n","100%|██████████| 1000/1000 [00:20<00:00, 48.55it/s]\n","100%|██████████| 1000/1000 [00:20<00:00, 49.83it/s]\n","100%|██████████| 1000/1000 [00:21<00:00, 47.10it/s]\n","100%|██████████| 1000/1000 [00:20<00:00, 49.75it/s]\n","100%|██████████| 1000/1000 [00:21<00:00, 47.52it/s]\n","100%|██████████| 1000/1000 [00:19<00:00, 50.54it/s]\n","100%|██████████| 1000/1000 [00:20<00:00, 47.70it/s]\n","100%|██████████| 1000/1000 [00:20<00:00, 49.64it/s]\n","100%|██████████| 1000/1000 [00:20<00:00, 49.86it/s]\n","100%|██████████| 1000/1000 [00:20<00:00, 48.26it/s]\n","100%|██████████| 1000/1000 [00:19<00:00, 50.19it/s]\n","100%|██████████| 1000/1000 [00:20<00:00, 47.69it/s]\n","100%|██████████| 1000/1000 [00:19<00:00, 51.12it/s]\n","100%|██████████| 1000/1000 [00:20<00:00, 49.21it/s]\n","100%|██████████| 1000/1000 [00:20<00:00, 48.92it/s]\n","100%|██████████| 1000/1000 [00:19<00:00, 50.84it/s]\n","100%|██████████| 1000/1000 [00:20<00:00, 47.68it/s]\n","100%|██████████| 1000/1000 [00:19<00:00, 50.97it/s]\n","100%|██████████| 1000/1000 [00:20<00:00, 48.33it/s]\n","100%|██████████| 1000/1000 [00:19<00:00, 50.36it/s]\n","100%|██████████| 1000/1000 [00:20<00:00, 49.80it/s]\n","100%|██████████| 1000/1000 [00:20<00:00, 48.86it/s]\n","100%|██████████| 1000/1000 [00:19<00:00, 50.62it/s]\n","100%|██████████| 1000/1000 [00:21<00:00, 47.44it/s]\n","100%|██████████| 1000/1000 [00:19<00:00, 50.57it/s]\n","100%|██████████| 1000/1000 [00:20<00:00, 47.80it/s]\n","100%|██████████| 1000/1000 [00:19<00:00, 50.00it/s]\n","100%|██████████| 1000/1000 [00:20<00:00, 49.08it/s]\n","100%|██████████| 1000/1000 [00:20<00:00, 49.55it/s]\n","100%|██████████| 1000/1000 [00:19<00:00, 50.28it/s]\n","100%|██████████| 1000/1000 [00:20<00:00, 48.71it/s]\n","100%|██████████| 1000/1000 [00:19<00:00, 50.68it/s]\n","100%|██████████| 1000/1000 [00:22<00:00, 45.07it/s]\n","100%|██████████| 1000/1000 [00:20<00:00, 49.20it/s]\n","100%|██████████| 1000/1000 [00:21<00:00, 46.25it/s]\n","100%|██████████| 1000/1000 [00:20<00:00, 49.64it/s]\n","100%|██████████| 1000/1000 [00:21<00:00, 46.85it/s]\n","100%|██████████| 1000/1000 [00:20<00:00, 49.34it/s]\n","100%|██████████| 1000/1000 [00:21<00:00, 46.41it/s]"]},{"output_type":"stream","name":"stdout","text":["Total training time: 2077.71 seconds\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"code","source":["for intermediates in list_inter:\n","  decoded_images = []\n","  for image in intermediates:\n","      with torch.no_grad():\n","          decoded_images.append(image)\n","  plt.figure(figsize=(10, 12))\n","  chain = torch.cat(decoded_images, dim=-1)\n","  plt.style.use(\"default\")\n","  plt.imshow(chain[0, 0].cpu(), vmin=0, vmax=1)\n","  plt.tight_layout()\n","  plt.axis(\"off\")"],"metadata":{"id":"yRDY6tLacAsA","colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1USCAKIoHUk4M_vq_dUxoXFHngl-t4A9B"},"executionInfo":{"status":"ok","timestamp":1734948200449,"user_tz":-420,"elapsed":28391,"user":{"displayName":"Hung Nguyen","userId":"03464478727849698728"}},"outputId":"b22a157b-2c3d-4e30-ce61-7c53bf9fa32d"},"execution_count":11,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"code","source":["from PIL import Image\n","os.makedirs(\"src_hand_all\", exist_ok=True)  # Đổi tên thư mục để phân biệt\n","selected_images = []\n","image_count = 0  # Thêm biến đếm số hình ảnh đã xử lý\n","\n","for i, batch in enumerate(val_loader):\n","    images = batch['image']\n","    for j in range(images.size(0)):\n","        selected_images.append(images[j])\n","        image = images[j].squeeze(0)\n","        image = image.cpu().numpy()\n","        image = (image * 255).astype(np.uint8)\n","        pil_image = Image.fromarray(image, mode=\"L\")\n","        pil_image.save(f\"src_hand_all/src_hand_pic_{image_count}.jpg\")\n","        image_count += 1  # Tăng biến đếm sau mỗi lần lưu thành công\n","\n","# Không cần kiểm tra số lượng hình ảnh được chọn nữa\n"],"metadata":{"id":"v-uCsZ4Fph5I"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from PIL import Image\n","import torchvision.transforms as transforms\n","os.makedirs(\"src_blood_all\", exist_ok=True)  # Đổi tên thư mục để phân biệt\n","selected_images = []\n","image_count = 0  # Thêm biến đếm số hình ảnh đã xử lý\n","\n","for i, batch in enumerate(val_loader):\n","    images = batch\n","    for j in range(images.size(0)):\n","        image = intermediates[-1].squeeze(0)\n","        transform = transforms.ToPILImage()\n","        pil_image = transform(image)\n","        pil_image.save(f\"src_blood_all/src_blood_pic_{image_count}.jpg\")\n","        image_count += 1  # Tăng biến đếm sau mỗi lần lưu thành công\n","\n","# Không cần kiểm tra số lượng hình ảnh được chọn nữa"],"metadata":{"id":"N6kcm_CRhsi1","executionInfo":{"status":"ok","timestamp":1734948333612,"user_tz":-420,"elapsed":12902,"user":{"displayName":"Hung Nguyen","userId":"03464478727849698728"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["os.makedirs(\"src_chest_100\", exist_ok=True)\n","selected_images = []\n","for i, images in enumerate(val_loader):\n","    if len(selected_images) >= 10:\n","        break\n","    for j in range(images.size(0)):\n","        selected_images.append(images[j])\n","        if len(selected_images) == 10:\n","            break\n","for i, image in enumerate(selected_images):\n","    image = image.squeeze(0)\n","    image = image.cpu().numpy()\n","    image = (image * 255).astype(np.uint8)\n","    pil_image = Image.fromarray(image, mode=\"L\")\n","    pil_image.save(f\"src_chest_100/src_chest_pic_{i}.jpg\")"],"metadata":{"id":"O1AJVfRgbDlx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from PIL import Image\n","import torchvision.transforms as transforms\n","os.makedirs(\"gen_blood_sep_100\", exist_ok=True)\n","i= 0\n","for intermediates in list_inter:\n","    image = intermediates[-1]\n","    print(\"Original shape:\", image.shape)\n","    image = intermediates[-1].squeeze(0)\n","    transform = transforms.ToPILImage()\n","    pil_image = transform(image)\n","    pil_image.save(f\"gen_blood_sep_100/gen_sep_pic_{i}.jpg\")\n","    i+=1"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eJy7MPhyBcFq","executionInfo":{"status":"ok","timestamp":1734948301269,"user_tz":-420,"elapsed":995,"user":{"displayName":"Hung Nguyen","userId":"03464478727849698728"}},"outputId":"2c80d9bb-04e2-43cc-8966-8b296bd7fcfd"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["Original shape: torch.Size([1, 3, 64, 64])\n","Original shape: torch.Size([1, 3, 64, 64])\n","Original shape: torch.Size([1, 3, 64, 64])\n","Original shape: torch.Size([1, 3, 64, 64])\n","Original shape: torch.Size([1, 3, 64, 64])\n","Original shape: torch.Size([1, 3, 64, 64])\n","Original shape: torch.Size([1, 3, 64, 64])\n","Original shape: torch.Size([1, 3, 64, 64])\n","Original shape: torch.Size([1, 3, 64, 64])\n","Original shape: torch.Size([1, 3, 64, 64])\n","Original shape: torch.Size([1, 3, 64, 64])\n","Original shape: torch.Size([1, 3, 64, 64])\n","Original shape: torch.Size([1, 3, 64, 64])\n","Original shape: torch.Size([1, 3, 64, 64])\n","Original shape: torch.Size([1, 3, 64, 64])\n","Original shape: torch.Size([1, 3, 64, 64])\n","Original shape: torch.Size([1, 3, 64, 64])\n","Original shape: torch.Size([1, 3, 64, 64])\n","Original shape: torch.Size([1, 3, 64, 64])\n","Original shape: torch.Size([1, 3, 64, 64])\n","Original shape: torch.Size([1, 3, 64, 64])\n","Original shape: torch.Size([1, 3, 64, 64])\n","Original shape: torch.Size([1, 3, 64, 64])\n","Original shape: torch.Size([1, 3, 64, 64])\n","Original shape: torch.Size([1, 3, 64, 64])\n","Original shape: torch.Size([1, 3, 64, 64])\n","Original shape: torch.Size([1, 3, 64, 64])\n","Original shape: torch.Size([1, 3, 64, 64])\n","Original shape: torch.Size([1, 3, 64, 64])\n","Original shape: torch.Size([1, 3, 64, 64])\n","Original shape: torch.Size([1, 3, 64, 64])\n","Original shape: torch.Size([1, 3, 64, 64])\n","Original shape: torch.Size([1, 3, 64, 64])\n","Original shape: torch.Size([1, 3, 64, 64])\n","Original shape: torch.Size([1, 3, 64, 64])\n","Original shape: torch.Size([1, 3, 64, 64])\n","Original shape: torch.Size([1, 3, 64, 64])\n","Original shape: torch.Size([1, 3, 64, 64])\n","Original shape: torch.Size([1, 3, 64, 64])\n","Original shape: torch.Size([1, 3, 64, 64])\n","Original shape: torch.Size([1, 3, 64, 64])\n","Original shape: torch.Size([1, 3, 64, 64])\n","Original shape: torch.Size([1, 3, 64, 64])\n","Original shape: torch.Size([1, 3, 64, 64])\n","Original shape: torch.Size([1, 3, 64, 64])\n","Original shape: torch.Size([1, 3, 64, 64])\n","Original shape: torch.Size([1, 3, 64, 64])\n","Original shape: torch.Size([1, 3, 64, 64])\n","Original shape: torch.Size([1, 3, 64, 64])\n","Original shape: torch.Size([1, 3, 64, 64])\n","Original shape: torch.Size([1, 3, 64, 64])\n","Original shape: torch.Size([1, 3, 64, 64])\n","Original shape: torch.Size([1, 3, 64, 64])\n","Original shape: torch.Size([1, 3, 64, 64])\n","Original shape: torch.Size([1, 3, 64, 64])\n","Original shape: torch.Size([1, 3, 64, 64])\n","Original shape: torch.Size([1, 3, 64, 64])\n","Original shape: torch.Size([1, 3, 64, 64])\n","Original shape: torch.Size([1, 3, 64, 64])\n","Original shape: torch.Size([1, 3, 64, 64])\n","Original shape: torch.Size([1, 3, 64, 64])\n","Original shape: torch.Size([1, 3, 64, 64])\n","Original shape: torch.Size([1, 3, 64, 64])\n","Original shape: torch.Size([1, 3, 64, 64])\n","Original shape: torch.Size([1, 3, 64, 64])\n","Original shape: torch.Size([1, 3, 64, 64])\n","Original shape: torch.Size([1, 3, 64, 64])\n","Original shape: torch.Size([1, 3, 64, 64])\n","Original shape: torch.Size([1, 3, 64, 64])\n","Original shape: torch.Size([1, 3, 64, 64])\n","Original shape: torch.Size([1, 3, 64, 64])\n","Original shape: torch.Size([1, 3, 64, 64])\n","Original shape: torch.Size([1, 3, 64, 64])\n","Original shape: torch.Size([1, 3, 64, 64])\n","Original shape: torch.Size([1, 3, 64, 64])\n","Original shape: torch.Size([1, 3, 64, 64])\n","Original shape: torch.Size([1, 3, 64, 64])\n","Original shape: torch.Size([1, 3, 64, 64])\n","Original shape: torch.Size([1, 3, 64, 64])\n","Original shape: torch.Size([1, 3, 64, 64])\n","Original shape: torch.Size([1, 3, 64, 64])\n","Original shape: torch.Size([1, 3, 64, 64])\n","Original shape: torch.Size([1, 3, 64, 64])\n","Original shape: torch.Size([1, 3, 64, 64])\n","Original shape: torch.Size([1, 3, 64, 64])\n","Original shape: torch.Size([1, 3, 64, 64])\n","Original shape: torch.Size([1, 3, 64, 64])\n","Original shape: torch.Size([1, 3, 64, 64])\n","Original shape: torch.Size([1, 3, 64, 64])\n","Original shape: torch.Size([1, 3, 64, 64])\n","Original shape: torch.Size([1, 3, 64, 64])\n","Original shape: torch.Size([1, 3, 64, 64])\n","Original shape: torch.Size([1, 3, 64, 64])\n","Original shape: torch.Size([1, 3, 64, 64])\n","Original shape: torch.Size([1, 3, 64, 64])\n","Original shape: torch.Size([1, 3, 64, 64])\n","Original shape: torch.Size([1, 3, 64, 64])\n","Original shape: torch.Size([1, 3, 64, 64])\n","Original shape: torch.Size([1, 3, 64, 64])\n","Original shape: torch.Size([1, 3, 64, 64])\n"]}]},{"cell_type":"code","source":["from PIL import Image\n","os.makedirs(\"gen_blood_moon_100\", exist_ok=True)\n","i = 0\n","\n","for intermediates in list_inter:\n","    image = intermediates[-1]\n","    image = image.squeeze(0).squeeze(0)\n","    image = image.cpu().numpy()\n","    image = (image * 255).astype(np.uint8)\n","    pil_image = Image.fromarray(image, mode=\"L\")\n","    pil_image.save(f\"gen_blood_moon_100/gen_moon_pic_{i}.jpg\")\n","    i += 1"],"metadata":{"id":"a25RGWD2dMtB","colab":{"base_uri":"https://localhost:8080/","height":332},"executionInfo":{"status":"error","timestamp":1734915474118,"user_tz":-420,"elapsed":8,"user":{"displayName":"Hung Nguyen","userId":"03464478727849698728"}},"outputId":"3d2040b3-5ff3-44ff-bc35-618de83c2b65"},"execution_count":null,"outputs":[{"output_type":"error","ename":"ValueError","evalue":"Too many dimensions: 3 > 2.","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-14-23ec45964187>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m255\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muint8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mpil_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfromarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"L\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mpil_image\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"gen_blood_moon_100/gen_moon_pic_{i}.jpg\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mi\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mfromarray\u001b[0;34m(obj, mode)\u001b[0m\n\u001b[1;32m   3328\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mndmax\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3329\u001b[0m         \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"Too many dimensions: {ndim} > {ndmax}.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3330\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3331\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3332\u001b[0m     \u001b[0msize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: Too many dimensions: 3 > 2."]}]},{"cell_type":"code","source":["import numpy as np\n","from tqdm import tqdm\n","import torch.nn.functional as F\n","from scipy import linalg\n","from torchvision import transforms\n","from torch.utils.data import Dataset, DataLoader\n","import torch\n","from PIL import Image\n","from INCEPTION import InceptionV3\n","import pathlib\n"],"metadata":{"id":"z1pVWgv056mx","executionInfo":{"status":"ok","timestamp":1734948997178,"user_tz":-420,"elapsed":362,"user":{"displayName":"Hung Nguyen","userId":"03464478727849698728"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","execution_count":16,"metadata":{"id":"asy5dgXo5E2L","executionInfo":{"status":"ok","timestamp":1734948999673,"user_tz":-420,"elapsed":3,"user":{"displayName":"Hung Nguyen","userId":"03464478727849698728"}}},"outputs":[],"source":["IMAGE_EXTENSIONS = {'jpg'}\n","\n","\n","class ImagePathDataset(Dataset):\n","    def __init__(self, files, transform=None):\n","        self.files = files\n","        # Only include ToTensor if not already in the provided transform\n","        if transform is None:\n","            self.transform = transforms.Compose([\n","                transforms.ToTensor()  # Default: Convert PIL image to tensor\n","            ])\n","        else:\n","            self.transform = transform  # Use provided transform as-is\n","\n","    def __len__(self):\n","        return len(self.files)\n","\n","    def __getitem__(self, i):\n","        path = self.files[i]\n","        img = Image.open(path).convert('RGB')  # Open the image as PIL\n","        img = self.transform(img)  # Apply transformations\n","        return img"]},{"cell_type":"code","source":["def get_activations(files, model, batch_size, dims, device='cpu'):\n","\n","    model.eval()\n","\n","    if batch_size > len(files):\n","        batch_size = len(files)\n","\n","    dataset = ImagePathDataset(files, transform=transforms.ToTensor())\n","    data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n","\n","    pred_arr = np.empty((len(files), dims))\n","    start_idx = 0\n","\n","    for batch in tqdm(data_loader):\n","        batch = batch.to(device)\n","\n","        with torch.inference_mode():\n","            pred = model(batch)[0]\n","\n","        if pred.size(2) != 1 or pred.size(3) != 1:\n","            pred = F.adaptive_avg_pool2d(pred, output_size=(1, 1))\n","\n","        pred = pred.squeeze(3).squeeze(2).cpu().numpy()\n","        pred_arr[start_idx:start_idx+pred.shape[0]] = pred\n","        start_idx = start_idx + pred.shape[0]\n","\n","    return pred_arr"],"metadata":{"id":"xMAAILGr5GIh","executionInfo":{"status":"ok","timestamp":1734949000427,"user_tz":-420,"elapsed":374,"user":{"displayName":"Hung Nguyen","userId":"03464478727849698728"}}},"execution_count":17,"outputs":[]},{"cell_type":"code","source":["def calculate_frechet_distance(mu1, mu2, sigma1, sigma2, eps=1e-6):\n","\n","    mu1 = np.atleast_1d(mu1)\n","    mu2 = np.atleast_1d(mu2)\n","\n","    sigma1 = np.atleast_2d(sigma1)\n","    sigma2 = np.atleast_2d(sigma2)\n","\n","    assert mu1.shape == mu2.shape, 'Training and test mean vectors have different lengths'\n","    assert sigma1.shape == sigma2.shape, 'Training and test covariances have different dimensions'\n","\n","    diff = mu1 - mu2\n","    covmean, _ = linalg.sqrtm(sigma1.dot(sigma2), disp=False)\n","\n","    if not np.isfinite(covmean).all():\n","        msg = ('fid calculation produces sigular product; adding %s to diagonal cov estimates') % eps\n","        print(msg)\n","        offset = np.eye(sigma1.shape[0]) * eps\n","        covmean = linalg.sqrtm((sigma1 + offset).dot(sigma2 + offset))\n","\n","    if np.iscomplexobj(covmean):\n","        if not np.allclose(np.diagonal(covmean).imag, 0, atol=1e-3):\n","            m = np.max(np.abs(covmean.imag))\n","            raise ValueError('Imaginary component {}'.format(m))\n","        covmean = covmean.real\n","\n","    tr_covmean = np.trace(covmean)\n","\n","    return diff.dot(diff) + np.trace(sigma1) + np.trace(sigma2) - 2 * tr_covmean"],"metadata":{"id":"7qoONyRr5HgY","executionInfo":{"status":"ok","timestamp":1734949001661,"user_tz":-420,"elapsed":2,"user":{"displayName":"Hung Nguyen","userId":"03464478727849698728"}}},"execution_count":18,"outputs":[]},{"cell_type":"code","source":["def calculate_activation_statistics(files, model, batch_size, dims, device='cpu'):\n","\n","    act = get_activations(files, model, batch_size, dims, device)\n","    mu = np.mean(act, axis=0)\n","    sigma = np.cov(act, rowvar=False)\n","\n","    return mu, sigma\n","\n","def compute_statistics_of_path(path, model, batch_size, dims, device='cpu'):\n","\n","    path = pathlib.Path(path)\n","    files = sorted([file for ext in IMAGE_EXTENSIONS for file in path.glob('*.{}'.format(ext))])\n","    mu, sigma = calculate_activation_statistics(files, model, batch_size, dims, device)\n","\n","    return mu, sigma\n","\n","def calculate_fid_given_paths(path1, path2, batch_size, dims, device='cpu'):\n","\n","    block_idx = InceptionV3.BLOCK_INDEX_BY_DIM[dims]\n","    print(block_idx)\n","    model = InceptionV3([block_idx]).to(device)\n","\n","    mu1, sigma1 = compute_statistics_of_path(path1, model, batch_size, dims, device)\n","    mu2, sigma2 = compute_statistics_of_path(path2, model, batch_size, dims, device)\n","\n","    fid_value = calculate_frechet_distance(mu1, mu2, sigma1, sigma2)\n","    return print('FID distance:', round(fid_value, 3))"],"metadata":{"id":"mzFrIiYU5Iwh","executionInfo":{"status":"ok","timestamp":1734949002826,"user_tz":-420,"elapsed":5,"user":{"displayName":"Hung Nguyen","userId":"03464478727849698728"}}},"execution_count":19,"outputs":[]},{"cell_type":"code","source":["device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","batch_size = 50\n","#BLOCK_INDEX_BY_DIM = {64: 0, 192: 1, 768: 2, 2048: 3}\n","dims = 768\n","\n","src_path = os.getcwd() + '/src_blood_all'\n","gen_path = os.getcwd() + '/gen_blood_moon_100'\n","\n","print('Total images in src_blood_all:', len(next(os.walk('src_blood_all'))[2]))\n","print('Total images in gen_blood_moon_100:', len(next(os.walk('gen_blood_moon_100'))[2]))\n","\n","calculate_fid_given_paths(path1=src_path, path2=gen_path, batch_size=batch_size, dims=dims, device=device)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mvE5KSiU5LZo","executionInfo":{"status":"ok","timestamp":1734949119839,"user_tz":-420,"elapsed":15267,"user":{"displayName":"Hung Nguyen","userId":"03464478727849698728"}},"outputId":"265fab68-11d1-49ce-fb71-23c303fe0ef0"},"execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":["Total images in src_blood_all: 1712\n","Total images in gen_blood_moon_100: 100\n","2\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 35/35 [00:11<00:00,  3.11it/s]\n","100%|██████████| 2/2 [00:02<00:00,  1.13s/it]\n"]},{"output_type":"stream","name":"stdout","text":["FID distance: 0.951\n"]}]}]}